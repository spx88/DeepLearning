import tensorflow as tf
import os


def tensorflow_demo():
    a = 2
    b = 3
    c = a + b
    print("普通运算的结果", c)

    # tensorflow实现加法运算

    a_t = tf.constant(2)
    b_t = tf.constant(3)
    c_t = a_t + b_t
    print("tensorflow加法运算的结果", c_t)

    # 开启会话
    with tf.Session() as sess:
        c_t_value = sess.run(c_t)
        print("c_t_value:", c_t_value)

    return None


def graph_demo():
    """图的演示"""
    a = 2
    b = 3
    c = a + b
    print("普通运算的结果", c)

    # tensorflow实现加法运算

    a_t = tf.constant(2)
    b_t = tf.constant(3)
    c_t = a_t + b_t
    print("a_t:\n", a_t)
    print("b_t:\n", b_t)
    print("tensorflow加法运算的结果", c_t)

    # 查看默认图
    # 方法1：调用方法
    default_g = tf.get_default_graph()
    print("default_g", default_g)
    print("a_t的图属性", a_t.graph)
    print("c_t的图属性", c_t.graph)

    # 自定义图
    new_g = tf.Graph()
    # 在自己的图中定义数据和操作
    with new_g.as_default():
        a_new = tf.constant(20)
        b_new = tf.constant(30)
        c_new = a_new + b_new
        print("c_new:\n", c_new)
        print("a_new的图属性", a_new.graph)
        print("c_new的图属性", c_new.graph)

    # 开启会话
    with tf.Session() as sess:
        c_t_value = sess.run(c_t)

        print("c_t_value:", c_t_value)
        print("sess的图属性", sess.graph)
        # 1)将图写入本地生成events文件
        tf.summary.FileWriter("./tmp/summary", graph=sess.graph)

    # 开启new_g的会话
    with tf.Session(graph=new_g) as new_sess:
        c_new_value = new_sess.run(c_new)
        print("c_new_value:\n", c_new_value)
        print("new_sess的图属性：\n", new_sess.graph)
    return None


def tensor_demo():
    # """张量的演示"""
    # 输入浮点型，默认是float32
    tensor1 = tf.constant(4.0)
    # 输入整型，默认是int32
    tensor2 = tf.constant([1, 2, 3, 4])
    linear_squares = tf.constant([[4], [9], [16], [25]], dtype=tf.int32)

    print("tensor1:", tensor1)
    print("tensor2:", tensor2)
    print("linear_squares_before", linear_squares)
    # 张量类型的修改
    l_cast = tf.cast(linear_squares, dtype=tf.float32)
    print("linear_squares_after:", linear_squares)
    print("l_cast", l_cast)

    # 更新/改变静态形状
    # 定义占位符
    a_p = tf.placeholder(dtype=tf.float32, shape=[None, None])
    b_p = tf.placeholder(dtype=tf.float32, shape=[None, 10])
    c_p = tf.placeholder(dtype=tf.float32, shape=[3, 2])
    print("update a_p before:", a_p)
    print("update b_p before", b_p)
    print("c_p", c_p)
    # 更新形状未确定的部分
    # a_p.set_shape([2, 3])
    # b_p.set_shape([2, 10])
    # 动态形状修改
    a_p_reshape = tf.reshape(a_p, shape=[2, 3, 1])
    print("a_p_after:", a_p)
    print("b_p_after", b_p)
    print("a_p_reshape", a_p_reshape)
    return None


def variable_demo():
    """变量的演示"""
    # 创建变量
    # 修改命名空间
    with tf.variable_scope("my_scope"):
        a = tf.Variable(initial_value=50)
        b = tf.Variable(initial_value=40)
    with tf.variable_scope('your_scope'):
        c = tf.add(a, b)
    print("a:", a)
    print("b:", b)
    print("c:", c)

    # 初始化变量
    init = tf.global_variables_initializer()

    # 开启会话
    with tf.Session() as sess:
        # 运行初始化
        sess.run(init)
        a_value, b_value, c_value = sess.run([a, b, c])
        print("a_value:", a_value)
        print("b_value:", b_value)
        print("c_value:", c_value)

    return None


# 1)定义命令行参数
tf.app.flags.DEFINE_integer("max_step", 100, "训练模型的步数")
tf.app.flags.DEFINE_string("model_dir", "Unknown", "模型保存的路径+模型名字")

# 2)简化变量名
FLAGS = tf.app.flags.FLAGS


def command_demo():
    """命令行演示"""
    print("max_step", FLAGS.max_step)
    print("model_dir", FLAGS.model_dir)

    return None


def linear_regression():
    """自实现一个线性回归"""

    with tf.variable_scope("prepare_data"):
        # 准备数据
        X = tf.random_normal(shape=[100, 1], name="feature")
        y_true = tf.matmul(X, [[0.8]]) + 0.7
    with tf.variable_scope("create_model"):
        # 构造模型
        # 定义模型参数用变量
        weights = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name="Weights")
        bias = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name="Bias")
        y_predict = tf.matmul(X, weights) + bias
    with tf.variable_scope("loss_function"):
        # 构造损失函数
        error = tf.reduce_mean(tf.square(y_predict - y_true))

    # 2 收集变量
    tf.summary.scalar("error", error)
    tf.summary.histogram("weights", weights)
    tf.summary.histogram("bias", bias)

    # 3 合并变量
    merged = tf.summary.merge_all()
    # 创建Saver对象
    saver = tf.train.Saver()

    with tf.variable_scope("optimizer"):
        # 优化损失
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)

    # 开启会话
    # 显示地初始化变量
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        # 初始化变量
        sess.run(init)

        # 1 创建事件文件
        file_writer = tf.summary.FileWriter("./tmp/linear", graph=sess.graph)

        print("训练前模型参数为：,权重%f,偏置%f, 损失为%f" % (weights.eval(), bias.eval(), error.eval()))

        # 开始训练
        # for i in range(100):
        #     sess.run(optimizer)
        #     print("第%d次训练后模型参数为：,权重%f,偏置%f, 损失为%f" % (i + 1, weights.eval(), bias.eval(), error.eval()))
        #
        #     # 运行合并变量操作
        #     summary = sess.run(merged)
        #     # 将每次迭代后的变量写入事件文件
        #     file_writer.add_summary(summary, i)
        #
        #     # 保存模型
        #     if i % 10 == 0:
        #         saver.save(sess, "./tmp/model/my_linear.ckpt")
        # 加载模型
        if os.path.exists("./tmp/model/checkpoint"):
            saver.restore(sess, "./tmp/model/my_linear.ckpt")

        print("训练后模型参数为：,权重%f,偏置%f, 损失为%f" % (weights.eval(), bias.eval(), error.eval()))

    return None


def main(argv):
    print(argv)
    return None


if __name__ == '__main__':
    # tensorflow_demo()
    # graph_demo()
    # tensor_demo()
    # variable_demo()
    # logistic回归实现
    # linear_regression()
    # 命令行参数演示
    # command_demo()
    tf.app.run()
